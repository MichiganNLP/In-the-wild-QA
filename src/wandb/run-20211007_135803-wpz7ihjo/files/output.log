  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Epoch 0:  33%|██████████████████████████████████▋                                                                     | 1/3 [00:00<00:00,  2.87it/s, loss=nan, v_num=ihjo]> /home/dnaihao/In-the-wild-QA/src/T5/model.py(81)training_epoch_end()
     80         import ipdb;ipdb.set_trace()
---> 81         avg_train_loss = torch.stack([x["loss"] for x in outputs]).mean()
     82         tensorboard_logs = {"avg_train_loss": avg_train_loss}
ipdb>
ipdb>
ipdb>
ipdb>
ipdb>
ipdb>
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
