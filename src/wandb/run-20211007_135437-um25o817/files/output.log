
Validation sanity check:   0%|                                                                                                                      | 0/2 [00:00<?, ?it/s]
  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "main.py", line 22, in <module>
    main()
  File "main.py", line 18, in main
    T5_train(args)
  File "/home/dnaihao/In-the-wild-QA/src/T5/train.py", line 57, in T5_train
    trainer.fit(model)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 513, in fit
    self.dispatch()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in dispatch
    self.accelerator.start_training(self)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 74, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 111, in start_training
    self._results = trainer.run_train()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 614, in run_train
    self.run_sanity_check(self.lightning_module)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 863, in run_sanity_check
    _, eval_results = self.run_evaluation(max_batches=self.num_sanity_val_batches)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 732, in run_evaluation
    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 164, in evaluation_step
    output = self.trainer.accelerator.validation_step(args)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 178, in validation_step
    return self.training_type_plugin.validation_step(*args)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 128, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/home/dnaihao/In-the-wild-QA/src/T5/model.py", line 87, in validation_step
    loss, acc, sent_acc = self._step(batch)
  File "/home/dnaihao/In-the-wild-QA/src/T5/model.py", line 53, in _step
    decoder_attention_mask=batch['target_mask']
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dnaihao/In-the-wild-QA/src/T5/model.py", line 42, in forward
    lm_labels=lm_labels,
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'lm_labels'