  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Validation sanity check:  50%|███████████████████████████████████████████████████████                                                       | 1/2 [00:00<00:00,  1.43it/s]> /home/dnaihao/In-the-wild-QA/src/T5/model.py(95)validation_epoch_end()
     94         import ipdb;ipdb.set_trace()
---> 95         avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
     96         tensorboard_logs = {"val_loss": avg_loss}


[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[ipdb> outputs
[?7h[?12l[?25h[?2004l[{'val_loss': tensor(6.8142, device='cuda:0')}, {'val_loss': tensor(13.8725, device='cuda:0')}]
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/In-the-wild-QA/src/T5/model.py(96)validation_epoch_end()
     95         avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
---> 96         tensorboard_logs = {"val_loss": avg_loss}
     97         # self.log('avg_val_loss', avg_loss)
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l--Return--
None
> /home/dnaihao/In-the-wild-QA/src/T5/model.py(96)validation_epoch_end()
     95         avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
---> 96         tensorboard_logs = {"val_loss": avg_loss}
     97         # self.log('avg_val_loss', avg_loss)
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(222)__run_eval_epoch_end()
    221                 eval_results = model.validation_epoch_end(eval_results)
--> 222                 user_reduced = True
    223
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(225)__run_eval_epoch_end()
    224         # capture logging
--> 225         self.trainer.logger_connector.cache_logged_metrics()
    226         # depre warning
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(227)__run_eval_epoch_end()
    226         # depre warning
--> 227         if eval_results is not None and user_reduced:
    228             step = 'testing_epoch_end' if self.trainer.testing else 'validation_epoch_end'
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(234)__run_eval_epoch_end()
    233
--> 234         if not isinstance(eval_results, list):
    235             eval_results = [eval_results]
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(235)__run_eval_epoch_end()
    234         if not isinstance(eval_results, list):
--> 235             eval_results = [eval_results]
    236
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(238)__run_eval_epoch_end()
    237         # track depreceated metrics
--> 238         self.trainer.logger_connector.track_metrics_deprecated(eval_results)
    239
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(240)__run_eval_epoch_end()
    239
--> 240         return eval_results
    241
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l--Return--
[None]
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(240)__run_eval_epoch_end()
    239
--> 240         return eval_results
    241
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(190)evaluation_epoch_end()
    189         # enable returning anything
--> 190         for i, r in enumerate(deprecated_results):
    191             if not isinstance(r, (dict, Result, torch.Tensor)):
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(191)evaluation_epoch_end()
    190         for i, r in enumerate(deprecated_results):
--> 191             if not isinstance(r, (dict, Result, torch.Tensor)):
    192                 deprecated_results[i] = []
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(192)evaluation_epoch_end()
    191             if not isinstance(r, (dict, Result, torch.Tensor)):
--> 192                 deprecated_results[i] = []
    193
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(190)evaluation_epoch_end()
    189         # enable returning anything
--> 190         for i, r in enumerate(deprecated_results):
    191             if not isinstance(r, (dict, Result, torch.Tensor)):
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(194)evaluation_epoch_end()
    193
--> 194         return deprecated_results
    195
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l--Return--
[[]]
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py(194)evaluation_epoch_end()
    193
--> 194         return deprecated_results
    195
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(751)run_evaluation()
    750         # hook
--> 751         self.evaluation_loop.on_evaluation_epoch_end()
    752
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(754)run_evaluation()
    753         # update epoch-level lr_schedulers
--> 754         if on_epoch:
    755             self.optimizer_connector.update_learning_rates(interval='epoch')
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
                                                           [?2ipdb>
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(761)run_evaluation()
    760         # log epoch metrics
--> 761         eval_loop_results = self.evaluation_loop.log_epoch_metrics_on_evaluation_end()
    762
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(764)run_evaluation()
    763         # save predictions to disk
--> 764         self.evaluation_loop.predictions.to_disk()
    765
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(767)run_evaluation()
    766         # enable train mode again
--> 767         self.evaluation_loop.on_evaluation_model_train()
    768
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(769)run_evaluation()
    768
--> 769         torch.set_grad_enabled(True)
    770
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(771)run_evaluation()
    770
--> 771         return eval_loop_results, deprecated_eval_results
    772
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l--Return--
([{}], [[]])
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(771)run_evaluation()
    770
--> 771         return eval_loop_results, deprecated_eval_results
    772
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(866)run_sanity_check()
    865             # allow no returns from eval
--> 866             if eval_results is not None and len(eval_results) > 0:
    867                 # when we get a list back, used only the last item
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
                                                           [?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(872)run_sanity_check()
    871                 _, _, _, callback_metrics, _ = self.process_dict_result(eval_results)
--> 872                 self.logger_connector.callback_metrics = callback_metrics
    873
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(874)run_sanity_check()
    873
--> 874             self.on_sanity_check_end()
    875             self.running_sanity_check = False
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(875)run_sanity_check()
    874             self.on_sanity_check_end()
--> 875             self.running_sanity_check = False
    876
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l--Return--
None
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(875)run_sanity_check()
    874             self.on_sanity_check_end()
--> 875             self.running_sanity_check = False
    876
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(617)run_train()
    616         # set stage for logging
--> 617         self._set_running_stage(RunningStage.TRAINING, self.lightning_module)
    618
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(619)run_train()
    618
--> 619         self.checkpoint_connector.has_trained = False
    620
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(622)run_train()
    621         # enable train mode
--> 622         model = self.lightning_module
    623         model.train()
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(623)run_train()
    622         model = self.lightning_module
--> 623         model.train()
    624         torch.set_grad_enabled(True)
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(624)run_train()
    623         model.train()
--> 624         torch.set_grad_enabled(True)
    625
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
                                                           [?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(630)run_train()
    629         # hook
--> 630         self.train_loop.on_train_start()
    631
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
Training: 0it [00:00, ?it/s]> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(632)run_train()
    631
--> 632         try:
    633             if self.train_loop.should_skip_training():
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(633)run_train()
    632         try:
--> 633             if self.train_loop.should_skip_training():
    634                 return
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(636)run_train()
    635             # run all epochs
--> 636             epochs = range(self.current_epoch, self.max_epochs) if self.max_epochs else count(self.current_epoch)
    637             for epoch in epochs:
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(637)run_train()
    636             epochs = range(self.current_epoch, self.max_epochs) if self.max_epochs else count(self.current_epoch)
--> 637             for epoch in epochs:
    638
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(640)run_train()
    639                 # hook
--> 640                 self.train_loop.on_train_epoch_start(epoch)
    641
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
Epoch 0:   0%|                                                                                                                                      | 0/3 [00:00<?, ?it/s]> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(642)run_train()
    641
--> 642                 with self.profiler.profile("run_training_epoch"):
    643                     # run train epoch
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(644)run_train()
    643                     # run train epoch
--> 644                     self.train_loop.run_training_epoch()
    645
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
Epoch 0:  33%|██████████████████████████████████▋                                                                     | 1/3 [00:00<00:01,  1.41it/s, loss=nan, v_num=o1d1]RuntimeError: stack expects a non-empty TensorList
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(644)run_train()
    643                     # run train epoch
--> 644                     self.train_loop.run_training_epoch()
    645
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(666)run_train()
    665
--> 666         except KeyboardInterrupt:
    667             rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(676)run_train()
    675             # hook
--> 676             self.train_loop.on_train_end()
    677
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
--Return--
None
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(676)run_train()
    675             # hook
--> 676             self.train_loop.on_train_end()
    677

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004lRuntimeError: stack expects a non-empty TensorList
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py(111)start_training()
    110         # double dispatch to initiate the training loop
--> 111         self._results = trainer.run_train()
    112

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lx[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lx[?7h[?12l[?25h[?25l[?7li[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25ipdb> exit
[?7h[?12l[?25h[?2004lExiting Debugger.