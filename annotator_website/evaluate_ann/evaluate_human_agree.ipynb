{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, Literal, Mapping\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")   # filter user warning for BLEU when overlap is 0\n",
    "\n",
    "\n",
    "class QAEvaluation:\n",
    "    def __init__(self, sources: list, preds: list, labels: list):\n",
    "        assert isinstance(labels[0], list)\n",
    "        self.sources = sources\n",
    "        self.preds = preds\n",
    "        self.labels = labels\n",
    "        assert len(self.sources) == len(self.preds) == len(self.labels)\n",
    "\n",
    "    def exact_match(self) -> float:\n",
    "        return sum(1 for pred, label in zip(self.preds, self.labels) if pred in label) / len(self.preds)\n",
    "\n",
    "    def bleu(self, n: Literal[1, 2, 3, 4]) -> float:\n",
    "        # individual BLEU n-gram score\n",
    "        pred_tokens = [word_tokenize(pred) for pred in self.preds]\n",
    "        label_tokens = [[word_tokenize(label) for label in l_labels] for l_labels in self.labels]\n",
    "\n",
    "        assert 1 <= n <= 4\n",
    "        weights = [0, 0, 0, 0]\n",
    "        weights[n - 1] = 1\n",
    "\n",
    "        return sum(sentence_bleu(label_tok, pred_tok, weights=tuple(weights))\n",
    "                   for pred_tok, label_tok in zip(pred_tokens, label_tokens)) / len(self.preds)\n",
    "\n",
    "    def rouge(self, n: Literal[1, 2, 3, 4, 5, \"l\"], t: Literal[\"n\", \"l\", \"w\"] = \"n\",\n",
    "              stats: Literal[\"p\", \"r\", \"f\"] = \"p\") -> float:\n",
    "        \"\"\"  \n",
    "        stats: \"p\": precision; \"r\": recall; \"f\": f1\n",
    "        t: Rouge type:\n",
    "            ROUGE-N: Overlap of N-grams between the system and reference summaries.\n",
    "            ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common \n",
    "                        subsequence problem takes into account sentence level structure\n",
    "                        similarity naturally and identifies longest co-occurring in \n",
    "                        sequence n-grams automatically.\n",
    "            ROUGE-W: Weighted LCS-based statistics that favors consecutive LCSes.\n",
    "        \"\"\"\n",
    "        assert n in {1, 2, 3, 4, 5, \"l\"}\n",
    "        evaluator = Rouge(metrics=[f\"rouge-{t}\"], max_n=n)\n",
    "        return sum(max(evaluator.get_scores(pred, label)[f\"rouge-{n}\"][stats] for label in labels)\n",
    "                   for pred, labels in zip(self.preds, self.labels)) / len(self.preds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for computing human performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, Literal, Mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_preds(preds: Iterable[str], labels: Iterable[str], maxs:bool=True) -> [float]:\n",
    "    \"\"\"\n",
    "    Evaluate scores for one person (with multiple scores)\n",
    "    Input:\n",
    "        preds: [pred1,pred2...]\n",
    "        labelss:[label1,label2..]\n",
    "        maxs: bool: (True: return maximum; False: return average)\n",
    "                    score for each metric\n",
    "    Output:\n",
    "        [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "    \"\"\"\n",
    "    evls = [QAEvaluation([\"-\"], [pred], [labels]) for pred in preds]\n",
    "    scores = [evl.exact_match() for evl in evls],\\\n",
    "                *[[evl.bleu(i+1) for evl in evls]for i in range(3)], \\\n",
    "                *[[evl.rouge(i+1) for evl in evls] for i in range(3)]\n",
    "    \n",
    "    if maxs: # return maximum score for each metric\n",
    "        return [max(s) for s in scores]\n",
    "    else: # return average score for each metric\n",
    "        return [np.average(s) for s in scores]\n",
    "\n",
    "\n",
    "def evaluate_one_question(predss: Iterable[Iterable[str]],stdAns:str=None,maxs:bool=True) -> [[float],[float]]:\n",
    "    '''\n",
    "    Input:\n",
    "        predss: humans' answers for one question\n",
    "        stdAns: standard answer for this question.\n",
    "        maxs: bool: (True: return maximum; False: return average)\n",
    "                    score for each metric\n",
    "    Output:\n",
    "        based on stdAnswer:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         ...]\n",
    "        based on leave-one-human-out:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         ...]\n",
    "    '''\n",
    "    assert stdAns or len(predss) >= 2, \"You have to provide either the standard answer \" +\\\n",
    "                                            \"or more then 2 humans' answers for evaluation.\"\n",
    "    \n",
    "    if len(predss) == 1: # only one person\n",
    "        return [evaluate_preds(predss[0],[stdAns],maxs=maxs)],[[]]\n",
    "    \n",
    "    stdScores = [] # stand answer scores\n",
    "    leavOneScores = [] # leave one human scores\n",
    "    for i in range(len(predss)): # for each human\n",
    "        leavOneLabel = [pred for j in range(len(predss)) if j!=i for pred in predss[j]]\n",
    "        if stdAns:\n",
    "            stdScores.append(evaluate_preds(predss[i],[stdAns],maxs=maxs))\n",
    "            leavOneLabel.append(stdAns)\n",
    "        else:\n",
    "            stdScores.append([])\n",
    "        leavOneScores.append(evaluate_preds(predss[i],leavOneLabel,maxs=maxs))\n",
    "    return stdScores, leavOneScores\n",
    "\n",
    "\n",
    "def evaluate_persons(answerss:Iterable[Iterable[Iterable[str]]],workerss:Iterable[Iterable[str]],\n",
    "                     stdAnswers:Iterable[str]=None, maxs:bool=True) -> [[float],[float]]:\n",
    "    '''\n",
    "    Compute human performance\n",
    "    Input:\n",
    "        answerss: human's answers for multiple questions\n",
    "        workerss: IDs for these human\n",
    "        stdAnswers: standard answer for these question\n",
    "        maxs: bool: (True: return maximum; False: return average)\n",
    "                    score for each metric\n",
    "    Output:\n",
    "        based on stdAnswer:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE3]\n",
    "        based on leave-one-human-out:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE3]\n",
    "    '''\n",
    "    \n",
    "    def eva_scores(stdLeaScores:[[[float],[float]]]) -> [[float],[float]]:\n",
    "        '''\n",
    "        Average scores for one person / for all persons\n",
    "        Input:\n",
    "            stdLeaScores: [[stdScores1,leaOneScores1],[stdScores2,leaOneScores2],..]\n",
    "        Output:\n",
    "            [aveStdScore,aveLeaScore]\n",
    "        '''\n",
    "        return [list(np.average(ps,axis=0)) if\n",
    "                (ps:=[si for s in stdLeaScores if (si:=s[i])!=[]])!=[] else []\n",
    "                for i in (0,1)]\n",
    "    \n",
    "    if stdAnswers is None:\n",
    "        stdAnswers = [None for _ in answerss]\n",
    "    \n",
    "    assert len(answerss) == len(workerss) \\\n",
    "        == len(stdAnswers), \"Length of 'answerss', 'workerss', 'stdAnswers' are not the same!\"\n",
    "    \n",
    "    person_scores = defaultdict(list)\n",
    "    for stdAns, answers, workers in zip(stdAnswers,answerss,workerss):\n",
    "        assert len(answers) == len(workers), \\\n",
    "            \"Number of wokers and answers are not the same for record:\\n\" + \\\n",
    "            f\"stdAns: {stdAns}\\nanswers:{answers}\\nworkers:{workers}\"\n",
    "        \n",
    "        std_s, lvo_s = evaluate_one_question(answers,stdAns=stdAns,maxs=maxs)\n",
    "        for stds,lvos,worker in zip(std_s,lvo_s,workers):\n",
    "            person_scores[worker].append([stds,lvos])\n",
    "    person_scores = dict(person_scores)\n",
    "    for k in person_scores.keys():\n",
    "        person_scores[k] = eva_scores(person_scores[k])\n",
    "    return eva_scores(list(person_scores.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_results = pd.read_csv(\"crowd_student_result.csv\",converters={\"answers\": lambda x:eval(x),\"workerids\": lambda x:eval(x)})\n",
    "org_ann = ann_results.groupby([\"question\",\"video\",\"stdAnswer\"]).\\\n",
    "    agg({\"answers\":list,\"workerids\":list}) # in case same video, same question, same stdAnswer in different HIT\n",
    "org_ann[[\"answers\",\"workerids\"]] = org_ann[[\"answers\",\"workerids\"]].apply(lambda x: [b for a in x for b in a])\n",
    "# TODO: one person answer the same question more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.19293399175286763,\n",
       "  0.028589166522782587,\n",
       "  0.007142857142857141,\n",
       "  0.26170801697117485,\n",
       "  0.028162393162393162,\n",
       "  0.008333333333333333],\n",
       " [0.0,\n",
       "  0.4093391284663095,\n",
       "  0.19290719225409358,\n",
       "  0.10301406240710845,\n",
       "  0.42270710560184244,\n",
       "  0.25085470085470085,\n",
       "  0.1511904761904762]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_persons(org_ann[\"answers\"],org_ann[\"workerids\"],\n",
    "                 stdAnswers=list(org_ann.index.get_level_values(2)),\n",
    "                 maxs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
