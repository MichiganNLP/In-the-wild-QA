
Epoch 0:  33%|██████████████████████████████████▋                                                                     | 1/3 [00:00<00:01,  1.58it/s, loss=nan, v_num=pl73]
  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "main.py", line 22, in <module>
    main()
  File "main.py", line 18, in main
    T5_train(args)
  File "/home/dnaihao/In-the-wild-QA/src/T5/train.py", line 57, in T5_train
    trainer.fit(model)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 513, in fit
    self.dispatch()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in dispatch
    self.accelerator.start_training(self)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 74, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 111, in start_training
    self._results = trainer.run_train()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 644, in run_train
    self.train_loop.run_training_epoch()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 559, in run_training_epoch
    epoch_output, self.checkpoint_accumulator, self.early_stopping_accumulator, self.num_optimizers
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 452, in log_train_epoch_end_metrics
    num_optimizers, epoch_output, model, is_result_obj, epoch_callback_metrics
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 527, in __run_legacy_training_epoch_end
    epoch_output = model.training_epoch_end(epoch_output)
  File "/home/dnaihao/In-the-wild-QA/src/T5/model.py", line 80, in training_epoch_end
    print(outputs)
RuntimeError: stack expects a non-empty TensorList