  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Epoch 0:   0%|                                                                                                                                      | 0/3 [00:00<?, ?it/s]> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(645)run_train()
    644                     import ipdb;ipdb.set_trace()
--> 645                     self.train_loop.run_training_epoch()
    646
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?ipdb> c
[?7h[?12l[?25h[?2004l> /home/dnaihao/In-the-wild-QA/src/T5/model.py(125)optimizer_step()
    124         import ipdb;ipdb.set_trace()
--> 125         assert not self.hparams.use_tpu
    126         optimizer.step()
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?ipdb> c
Epoch 0:  33%|██████████████████████████████████▋                                                                     | 1/3 [00:12<00:24, 12.18s/it, loss=nan, v_num=zlrw]
Traceback (most recent call last):
  File "main.py", line 22, in <module>
    main()
  File "main.py", line 18, in main
    T5_train(args)
  File "/home/dnaihao/In-the-wild-QA/src/T5/train.py", line 57, in T5_train
    trainer.fit(model)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 513, in fit
    self.dispatch()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in dispatch
    self.accelerator.start_training(self)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 74, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 111, in start_training
    self._results = trainer.run_train()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 645, in run_train
    self.train_loop.run_training_epoch()
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 559, in run_training_epoch
    epoch_output, self.checkpoint_accumulator, self.early_stopping_accumulator, self.num_optimizers
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 452, in log_train_epoch_end_metrics
    num_optimizers, epoch_output, model, is_result_obj, epoch_callback_metrics
  File "/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 527, in __run_legacy_training_epoch_end
    epoch_output = model.training_epoch_end(epoch_output)
  File "/home/dnaihao/In-the-wild-QA/src/T5/model.py", line 80, in training_epoch_end
    avg_train_loss = torch.stack([x["loss"] for x in outputs]).mean()
RuntimeError: stack expects a non-empty TensorList
If you suspect this is an IPython 7.18.1 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org
You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.
Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True