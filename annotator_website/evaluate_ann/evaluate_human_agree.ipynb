{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluations.evaluations import QAEvaluation, EvidenceEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for computing human performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, Literal, Mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_preds(preds: Iterable, labels: Iterable, \n",
    "                   obj:str=\"answer\", method:str=\"max\") -> [float]:\n",
    "    \"\"\"\n",
    "    Evaluate scores for one person (answer / evidence)\n",
    "    Input:\n",
    "        preds: [pred1,pred2...]\n",
    "        labels:[label1,label2..]\n",
    "        obj:\"answer\":calculate answer agreement;\n",
    "               \"evidence\":calculate evidence agreement;\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "      For answer: \n",
    "        [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "      For evidence: \n",
    "        [iou_f1]\n",
    "    \"\"\"\n",
    "    if obj == \"answer\":\n",
    "        evls = [QAEvaluation([\"-\"], [pred], [labels]) for pred in preds]\n",
    "        scores = [evl.exact_match() for evl in evls],\\\n",
    "                    *[[evl.bleu(i+1) for evl in evls]for i in range(3)], \\\n",
    "                    *[[evl.rouge(i+1) for evl in evls] for i in range(3)]\n",
    "    elif obj == \"evidence\":\n",
    "        scores = [max([EvidenceEvaluation([pred],[label]).iou_f1() \n",
    "                       for label in labels]) for pred in preds]\n",
    "    else:\n",
    "        raise ValueError(f\"no obj named as \\\"{obj}\\\"\")\n",
    "    \n",
    "    if method==\"max\": # return maximum score for each metric\n",
    "        return [max(s) for s in scores] if obj == \"answer\" else [max(scores)]\n",
    "    elif method==\"ave\":\n",
    "        return [np.average(s) for s in scores] if obj == \"answer\" else [np.average(scores)]\n",
    "    elif method==\"first\":\n",
    "        return [s[0] for s in scores] if obj == \"answer\" else [scores[0]]\n",
    "    else:\n",
    "        raise ValueError(f\"no method named as \\\"{method}\\\"\")\n",
    "\n",
    "\n",
    "def evaluate_one_question(predss: Iterable[Iterable],stdLabel=None,obj:str=\"answer\",\n",
    "                          method:str=\"max\") -> [[float],[float]]:\n",
    "    '''\n",
    "    Input:\n",
    "        predss: humans' answers for one question\n",
    "        stdLabel: standard label for this question.\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "      For answer: \n",
    "        based on stdLabel:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         ...]\n",
    "        based on leave-one-human-out:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE3]\n",
    "         ...]\n",
    "      For evidence:\n",
    "        based on stdLabel:\n",
    "        [person 1: [iou_f1], person 2: [iou_f1], ...]\n",
    "        based on leave-one-human-out:\n",
    "        [person 1: [iou_f1], person 2: [iou_f1], ...]\n",
    "    '''\n",
    "    assert stdLabel or len(predss) >= 2, \"You have to provide either the standard answer \" +\\\n",
    "                                            \"or more then 2 humans' answers for evaluation.\"\n",
    "    \n",
    "    if len(predss) == 1: # only one person\n",
    "        return [evaluate_preds(predss[0],[stdLabel],obj=obj,method=method)],[[]]\n",
    "    \n",
    "    stdScores = [] # stand answer scores\n",
    "    leavOneScores = [] # leave one human scores\n",
    "    for i in range(len(predss)): # for each human\n",
    "        leavOneLabel = [pred for j in range(len(predss)) if j!=i for pred in predss[j]]\n",
    "        if stdLabel:\n",
    "            stdScores.append(evaluate_preds(predss[i],[stdLabel],obj=obj,method=method))\n",
    "            leavOneLabel.append(stdLabel)\n",
    "        else:\n",
    "            stdScores.append([])\n",
    "        leavOneScores.append(evaluate_preds(predss[i],leavOneLabel,obj=obj,method=method))\n",
    "    return stdScores, leavOneScores\n",
    "\n",
    "\n",
    "def evaluate_persons(answerss:Iterable[Iterable[Iterable[str]]],workerss:Iterable[Iterable[str]],\n",
    "                     stdLabels:Iterable[str]=None, obj:str=\"answer\", method:str=\"max\") -> [[float],[float]]:\n",
    "    '''\n",
    "    Compute human performance\n",
    "    Input:\n",
    "        answerss: human's answers / evidences for multiple questions\n",
    "        workerss: IDs for these human\n",
    "        stdLabels: standard answer / evidence for these question\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "        based on stdLabel:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE3]\n",
    "        based on leave-one-human-out:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE3]\n",
    "    '''\n",
    "    \n",
    "    def eva_scores(stdLeaScores:[[[float],[float]]]) -> [[float],[float]]:\n",
    "        '''\n",
    "        Average scores for one person / for all persons\n",
    "        Input:\n",
    "            stdLeaScores: [[stdScores1,leaOneScores1],[stdScores2,leaOneScores2],..]\n",
    "        Output:\n",
    "            [aveStdScore,aveLeaScore]\n",
    "        '''\n",
    "        return [list(np.average(ps,axis=0)) if\n",
    "                (ps:=[si for s in stdLeaScores if (si:=s[i])!=[]])!=[] else []\n",
    "                for i in (0,1)]\n",
    "    \n",
    "    if stdLabels is None:\n",
    "        stdLabels = [None for _ in answerss]\n",
    "    \n",
    "    assert len(answerss) == len(workerss) \\\n",
    "        == len(stdLabels), \"Length of 'answerss', 'workerss', 'stdLabels' are not the same!\"\n",
    "    \n",
    "    person_scores = defaultdict(list)\n",
    "    for stdLabel, answers, workers in zip(stdLabels,answerss,workerss):\n",
    "        assert len(answers) == len(workers), \\\n",
    "            \"Number of wokers and answers are not the same for record:\\n\" + \\\n",
    "            f\"stdLabel: {stdLabel}\\nanswers:{answers}\\nworkers:{workers}\"\n",
    "        \n",
    "        std_s, lvo_s = evaluate_one_question(answers,stdLabel=stdLabel,obj=obj,method=method)\n",
    "        for stds,lvos,worker in zip(std_s,lvo_s,workers):\n",
    "            person_scores[worker].append([stds,lvos])\n",
    "    person_scores = dict(person_scores)\n",
    "    for k in person_scores.keys():\n",
    "        person_scores[k] = eva_scores(person_scores[k])\n",
    "    return eva_scores(list(person_scores.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_results = pd.read_csv(\"ans-exp-5-6-crowd.csv\",converters={\"answers\": lambda x:eval(x),\"workerids\": lambda x:eval(x),\n",
    "                                                        \"stdEvidences\":lambda x:eval(x), \"evidences\":lambda x:eval(x)})\n",
    "\n",
    "org_ann = ann_results.groupby([\"question\",\"video\",\"stdAnswer\"]).\\\n",
    "    agg({\"answers\":list,\"workerids\":list,\"stdEvidences\":list,\n",
    "        \"evidences\":list}) # in case same video, same question, same stdAnswer in different HIT\n",
    "org_ann[[\"answers\",\"workerids\",\"stdEvidences\",\"evidences\"]] =\\\n",
    "    org_ann[[\"answers\",\"workerids\",\"stdEvidences\",\"evidences\"]].apply(lambda x: [b for a in x for b in a])\n",
    "# TODO: one person answer the same question more than once\n",
    "\n",
    "org_ann[\"stdEvidences\"] = org_ann[\"stdEvidences\"].apply(lambda x : [list(t.values())[0] for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 19691.57it/s]\n",
      "1it [00:00, 18477.11it/s]\n",
      "1it [00:00, 20360.70it/s]\n",
      "1it [00:00, 25731.93it/s]\n",
      "1it [00:00, 11554.56it/s]\n",
      "1it [00:00, 12122.27it/s]\n",
      "1it [00:00, 12595.51it/s]\n",
      "1it [00:00, 23967.45it/s]\n",
      "1it [00:00, 9915.61it/s]\n",
      "1it [00:00, 10837.99it/s]\n",
      "1it [00:00, 24244.53it/s]\n",
      "1it [00:00, 24818.37it/s]\n",
      "1it [00:00, 9939.11it/s]\n",
      "1it [00:00, 23831.27it/s]\n",
      "1it [00:00, 11554.56it/s]\n",
      "1it [00:00, 6307.22it/s]\n",
      "1it [00:00, 13486.51it/s]\n",
      "1it [00:00, 5882.61it/s]\n",
      "1it [00:00, 8355.19it/s]\n",
      "1it [00:00, 11155.06it/s]\n",
      "1it [00:00, 11397.57it/s]\n",
      "1it [00:00, 9962.72it/s]\n",
      "1it [00:00, 8004.40it/s]\n",
      "1it [00:00, 9532.51it/s]\n",
      "1it [00:00, 11814.94it/s]\n",
      "1it [00:00, 12264.05it/s]\n",
      "1it [00:00, 10082.46it/s]\n",
      "1it [00:00, 13617.87it/s]\n",
      "1it [00:00, 11781.75it/s]\n",
      "1it [00:00, 11459.85it/s]\n",
      "1it [00:00, 7738.57it/s]\n",
      "1it [00:00, 5216.80it/s]\n",
      "1it [00:00, 8192.00it/s]\n",
      "1it [00:00, 6594.82it/s]\n",
      "1it [00:00, 10330.80it/s]\n",
      "1it [00:00, 11125.47it/s]\n",
      "1it [00:00, 11096.04it/s]\n",
      "1it [00:00, 14122.24it/s]\n",
      "1it [00:00, 10951.19it/s]\n",
      "1it [00:00, 13357.66it/s]\n",
      "1it [00:00, 11848.32it/s]\n",
      "1it [00:00, 12595.51it/s]\n",
      "1it [00:00, 13315.25it/s]\n",
      "1it [00:00, 3287.07it/s]\n",
      "1it [00:00, 9554.22it/s]\n",
      "1it [00:00, 10106.76it/s]\n",
      "1it [00:00, 12826.62it/s]\n",
      "1it [00:00, 9137.92it/s]\n",
      "1it [00:00, 5737.76it/s]\n",
      "1it [00:00, 13530.01it/s]\n",
      "1it [00:00, 8322.03it/s]\n",
      "1it [00:00, 12557.80it/s]\n",
      "1it [00:00, 13888.42it/s]\n",
      "1it [00:00, 11008.67it/s]\n",
      "1it [00:00, 10866.07it/s]\n",
      "1it [00:00, 10618.49it/s]\n",
      "1it [00:00, 12372.58it/s]\n",
      "1it [00:00, 11881.88it/s]\n",
      "1it [00:00, 6364.65it/s]\n",
      "1it [00:00, 12985.46it/s]\n",
      "1it [00:00, 12865.96it/s]\n",
      "1it [00:00, 17260.51it/s]\n",
      "1it [00:00, 18396.07it/s]\n",
      "1it [00:00, 16710.37it/s]\n",
      "1it [00:00, 17623.13it/s]\n",
      "1it [00:00, 17549.39it/s]\n",
      "1it [00:00, 12300.01it/s]\n",
      "1it [00:00, 18893.26it/s]\n",
      "1it [00:00, 7781.64it/s]\n",
      "1it [00:00, 18477.11it/s]\n",
      "1it [00:00, 22192.08it/s]\n",
      "1it [00:00, 9915.61it/s]\n",
      "1it [00:00, 12446.01it/s]\n",
      "1it [00:00, 17623.13it/s]\n",
      "1it [00:00, 17924.38it/s]\n",
      "1it [00:00, 14820.86it/s]\n",
      "1it [00:00, 18558.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Agreement: [[0.0, 0.11552273887539768, 0.012810131195534084, 0.005890236416116417, 0.1854206532392807, 0.027396611496535217, 0.006891133557800224], []]\n",
      "\n",
      "Evidence Agreement: [[0.14740740740740743], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "METHOD = \"first\"\n",
    "\n",
    "AnsAgree = evaluate_persons(org_ann[\"answers\"],org_ann[\"workerids\"],\n",
    "                 stdLabels=list(org_ann.index.get_level_values(2)),\n",
    "                 obj=\"answer\",method=METHOD)\n",
    "\n",
    "EvdAgree = evaluate_persons(org_ann[\"evidences\"],org_ann[\"workerids\"],\n",
    "                 stdLabels=org_ann[\"stdEvidences\"],\n",
    "                 obj=\"evidence\",method=METHOD)\n",
    "\n",
    "print(f\"Answer Agreement: {AnsAgree}\\n\\nEvidence Agreement: {EvdAgree}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_tensorflow1]",
   "language": "python",
   "name": "conda-env-pytorch_tensorflow1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
