{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluations import QAEvaluation, EvidenceEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for computing human performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, Literal, Mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_preds(preds: Iterable, labels: Iterable, \n",
    "                   obj:str=\"answer\", iou_threshold:float=0.5, method:str=\"max\") -> [float]:\n",
    "    \"\"\"\n",
    "    Evaluate scores for one person (answer / evidence)\n",
    "    Input:\n",
    "        preds: [pred1,pred2...]\n",
    "        labels:[label1,label2..]\n",
    "        obj:\"answer\":calculate answer agreement;\n",
    "               \"evidence\":calculate evidence agreement;\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "      For answer: \n",
    "        [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE-L]\n",
    "      For evidence: \n",
    "        [iou_f1]\n",
    "    \"\"\"\n",
    "    if obj == \"answer\":\n",
    "        evls = [QAEvaluation([\"-\"], [pred], [labels]) for pred in preds]\n",
    "        scores = [evl.exact_match() for evl in evls],\\\n",
    "                    *[[evl.bleu(i+1) for evl in evls]for i in range(3)], \\\n",
    "                    *[[evl.rouge(k,stats=\"f\") for evl in evls] for k in [1,2,\"l\"]]\n",
    "    elif obj == \"evidence\":\n",
    "        scores = [max([EvidenceEvaluation([pred],[label]).iou_f1(threshold=iou_threshold)\n",
    "                       for label in labels]) for pred in preds]\n",
    "    else:\n",
    "        raise ValueError(f\"no obj named as \\\"{obj}\\\"\")\n",
    "    \n",
    "    if method==\"max\": # return maximum score for each metric\n",
    "        return [max(s) for s in scores] if obj == \"answer\" else [max(scores)]\n",
    "    elif method==\"ave\":\n",
    "        return [np.average(s) for s in scores] if obj == \"answer\" else [np.average(scores)]\n",
    "    elif method==\"first\":\n",
    "        return [s[0] for s in scores] if obj == \"answer\" else [scores[0]]\n",
    "    else:\n",
    "        raise ValueError(f\"no method named as \\\"{method}\\\"\")\n",
    "\n",
    "\n",
    "def evaluate_one_question(predss: Iterable[Iterable],stdLabel=None,obj:str=\"answer\",\n",
    "                          iou_threshold:float=0.5,method:str=\"max\") -> [[float],[float]]:\n",
    "    '''\n",
    "    Input:\n",
    "        predss: humans' answers for one question\n",
    "        stdLabel: standard label for this question.\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "      For answer: \n",
    "        based on stdLabel:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE-L]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE-L]\n",
    "         ...]\n",
    "        based on leave-one-human-out:\n",
    "        [person 1: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE-L]\n",
    "         person 2: [exact_match, BLEU1, BLEU2, BLEU3, ROUGE1, ROUGE2, ROUGE-L]\n",
    "         ...]\n",
    "      For evidence:\n",
    "        based on stdLabel:\n",
    "        [person 1: [iou_f1], person 2: [iou_f1], ...]\n",
    "        based on leave-one-human-out:\n",
    "        [person 1: [iou_f1], person 2: [iou_f1], ...]\n",
    "    '''\n",
    "    assert stdLabel or len(predss) >= 2, \"You have to provide either the standard answer \" +\\\n",
    "                                            \"or more then 2 humans' answers for evaluation.\"\n",
    "    \n",
    "    if len(predss) == 1: # only one person\n",
    "        return [evaluate_preds(predss[0],[stdLabel],obj=obj,\n",
    "                               iou_threshold=iou_threshold,method=method)],[[]]\n",
    "    \n",
    "    stdScores = [] # stand answer scores\n",
    "    leavOneScores = [] # leave one human scores\n",
    "    for i in range(len(predss)): # for each human\n",
    "        leavOneLabel = [pred for j in range(len(predss)) if j!=i for pred in predss[j]]\n",
    "        if stdLabel:\n",
    "            stdScores.append(evaluate_preds(predss[i],[stdLabel],obj=obj,\n",
    "                                            iou_threshold=iou_threshold,method=method))\n",
    "            leavOneLabel.append(stdLabel)\n",
    "        else:\n",
    "            stdScores.append([])\n",
    "        leavOneScores.append(evaluate_preds(predss[i],leavOneLabel,obj=obj,\n",
    "                                            iou_threshold=iou_threshold,method=method))\n",
    "    return stdScores, leavOneScores\n",
    "\n",
    "\n",
    "def evaluate_persons(answerss:Iterable[Iterable[Iterable[str]]],workerss:Iterable[Iterable[str]],\n",
    "                     stdLabels:Iterable[str]=None, obj:str=\"answer\", \n",
    "                     iou_threshold:float=0.5,method:str=\"max\") -> [[float],[float]]:\n",
    "    '''\n",
    "    Compute human performance\n",
    "    Input:\n",
    "        answerss: human's answers / evidences for multiple questions\n",
    "        workerss: IDs for these human\n",
    "        stdLabels: standard answer / evidence for these question\n",
    "        method:\"max\": return the maximum score;\n",
    "               \"ave\": return the average score;\n",
    "               \"first\": return score of the first answer\n",
    "                      for each metric\n",
    "    Output:\n",
    "        based on stdLabel:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE-L]\n",
    "        based on leave-one-human-out:\n",
    "        [ave_exact_match, ave_BLEU1, ave_BLEU2, ave_BLEU3, ave_ROUGE1, ave_ROUGE2, ave_ROUGE-L]\n",
    "    '''\n",
    "    \n",
    "    def eva_scores(stdLeaScores:[[[float],[float]]]) -> [[float],[float]]:\n",
    "        '''\n",
    "        Average scores for one person / for all persons\n",
    "        Input:\n",
    "            stdLeaScores: [[stdScores1,leaOneScores1],[stdScores2,leaOneScores2],..]\n",
    "        Output:\n",
    "            [aveStdScore,aveLeaScore]\n",
    "        '''\n",
    "        return [list(np.average(ps,axis=0)) if\n",
    "                (ps:=[si for s in stdLeaScores if (si:=s[i])!=[]])!=[] else []\n",
    "                for i in (0,1)]\n",
    "    \n",
    "    if stdLabels is None:\n",
    "        stdLabels = [None for _ in answerss]\n",
    "    \n",
    "    assert len(answerss) == len(workerss) \\\n",
    "        == len(stdLabels), \"Length of 'answerss', 'workerss', 'stdLabels' are not the same!\"\n",
    "    \n",
    "    person_scores = defaultdict(list)\n",
    "    for stdLabel, answers, workers in zip(stdLabels,answerss,workerss):\n",
    "        assert len(answers) == len(workers), \\\n",
    "            \"Number of wokers and answers are not the same for record:\\n\" + \\\n",
    "            f\"stdLabel: {stdLabel}\\nanswers:{answers}\\nworkers:{workers}\"\n",
    "        \n",
    "        std_s, lvo_s = evaluate_one_question(answers,stdLabel=stdLabel,obj=obj,\n",
    "                                             iou_threshold=iou_threshold,method=method)\n",
    "        for stds,lvos,worker in zip(std_s,lvo_s,workers):\n",
    "            person_scores[worker].append([stds,lvos])\n",
    "    person_scores = dict(person_scores)\n",
    "    for k in person_scores.keys():\n",
    "        person_scores[k] = eva_scores(person_scores[k])\n",
    "    return eva_scores(list(person_scores.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_results = pd.read_csv(\"../organize_stage2_annotation/merged_annotation.csv\",\n",
    "                          converters={cln: lambda x:eval(x) for cln in \n",
    "                                      [\"evidences\",'crowd_answers', 'crowd_evidences','crowd_deleted_evidences','crowd_workerids',\n",
    "                                        'expert_answers', 'expert_evidences','expert_deleted_evidences','expert_workerids']})\n",
    "ann_results = ann_results[ann_results[\"crowd_answers\"].apply(lambda x: x !=[])] # remove unannotated (in phase2) questions\n",
    "ann_results = ann_results[[\n",
    "    'modified_question', 'modified_answer','evidences', 'domain','video_link',\n",
    "    'crowd_answers', 'crowd_evidences','crowd_deleted_evidences','crowd_workerids',\n",
    "    'expert_answers', 'expert_evidences','expert_deleted_evidences','expert_workerids']]\n",
    "ann_results[\"evidences\"] = ann_results[\"evidences\"].apply(lambda x : [list(t.values())[0] for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: boost speed\n",
    "import copy\n",
    "\n",
    "def union_workers(x):\n",
    "    '''\n",
    "    put crowd and expert annotations together\n",
    "    '''\n",
    "    return x[\"crowd_answers\"]+x[\"expert_answers\"],\\\n",
    "        x[\"crowd_evidences\"]+x[\"expert_evidences\"],\\\n",
    "        x[\"crowd_workerids\"]+x[\"expert_workerids\"]\n",
    "\n",
    "\n",
    "def remove_empty_evid(x):\n",
    "    new_combine_evidences = copy.deepcopy(x[\"combine_evidences\"])\n",
    "    new_combine_workerids = copy.deepcopy(x[\"combine_workerids\"])\n",
    "    for i in range(len(x[\"combine_answers\"])-1,-1,-1):\n",
    "        if x[\"combine_evidences\"][i] == [[]]:\n",
    "            del new_combine_evidences[i]\n",
    "            del new_combine_workerids[i]\n",
    "    return new_combine_evidences,new_combine_workerids\n",
    "\n",
    "\n",
    "def fill_empty_evid(x):\n",
    "    new_combine_evidences = copy.deepcopy(x[\"combine_evidences\"])\n",
    "    for i in range(len(x[\"combine_answers\"])):\n",
    "        if x[\"combine_evidences\"][i] == [[]]:\n",
    "            new_combine_evidences[i][0].append([-2,-3])\n",
    "    return new_combine_evidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As multiple workers\n",
    "Calculate scores for each worker, then average among workers \\\n",
    "Compared with `as one worker`: view all workers as one worker, and merge their annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_results[[\"combine_answers\",\"combine_evidences\",\"combine_workerids\"]]=\\\n",
    "    ann_results.apply(union_workers,axis=1,result_type='expand') # union crowd and expert answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Agreement: [[1.2, 19.16, 6.83, 3.72, 26.32, 9.63, 23.72], [0.98, 44.17, 16.15, 7.7, 42.41, 19.14, 37.82]]\n",
      "skip Evidence Agreement: [[11.77], [14.34]]\n",
      "zero Evidence Agreement: [[6.31], [7.23]]\n"
     ]
    }
   ],
   "source": [
    "METHOD = \"first\"\n",
    "# method:\"max\": return the maximum score;\n",
    "#        \"ave\": return the average score;\n",
    "#        \"first\": return score of the first answer\n",
    "#                 for each metric\n",
    "\n",
    "# Answer Agreement\n",
    "AnsAgree = evaluate_persons(ann_results[\"combine_answers\"],ann_results[\"combine_workerids\"],\n",
    "                 stdLabels=ann_results[\"modified_answer\"],\n",
    "                 obj=\"answer\",method=METHOD)\n",
    "\n",
    "print(f\"Answer Agreement: {[[round(b*100,2) for b in A]for A in AnsAgree]}\")\n",
    "\n",
    "\n",
    "# Evidence Agreement\n",
    "\n",
    "## skip empty evidences\n",
    "evd_input_df = copy.deepcopy(ann_results[[\"evidences\"]])\n",
    "# remove empty evidences and corresponding workers\n",
    "evd_input_df[[\"combine_evidences\",\"combine_workerids\"]]=\\\n",
    "    ann_results.apply(remove_empty_evid,axis=1,result_type='expand')\n",
    "evd_input_df = evd_input_df[evd_input_df[\"combine_workerids\"].apply(lambda x:x!=[])]\n",
    "\n",
    "EvdAgree = evaluate_persons(evd_input_df[\"combine_evidences\"],evd_input_df[\"combine_workerids\"],\n",
    "                 stdLabels=evd_input_df[\"evidences\"],\n",
    "                 obj=\"evidence\",iou_threshold=0.5, method=METHOD)\n",
    "print(f\"skip Evidence Agreement: {[[round(b*100,2) for b in A] for A in EvdAgree]}\")\n",
    "\n",
    "    \n",
    "## score empty evidences as 0\n",
    "evd_input_df = copy.deepcopy(ann_results[[\"evidences\",\"combine_workerids\"]])\n",
    "evd_input_df[\"combine_evidences\"]=\\\n",
    "    ann_results.apply(fill_empty_evid,axis=1) # remove empty evidences and according workers\n",
    "\n",
    "EvdAgree = evaluate_persons(evd_input_df[\"combine_evidences\"],evd_input_df[\"combine_workerids\"],\n",
    "                 stdLabels=evd_input_df[\"evidences\"],\n",
    "                 obj=\"evidence\",method=METHOD)\n",
    "\n",
    "print(f\"zero Evidence Agreement: {[[round(b*100,2) for b in A] for A in EvdAgree]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as one workers (depricated, decide to use <u>multiple workers</u> strategy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_results[[\"combine_answers\",\"combine_evidences\",\"combine_workerids\"]]=\\\n",
    "#     ann_results.apply(union_workers,axis=1,result_type='expand') # union crowd and expert answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed=0\n",
    "# def select_worker(x):\n",
    "#     i = random.randint(0,len(x[\"combine_answers\"])-1)\n",
    "#     return x[\"combine_answers\"][i:i+1],x[\"combine_evidences\"][i:i+1],[\"OnePerson\"]\n",
    "\n",
    "# ann_results[[\"combine_answers\",\"combine_evidences\",\"combine_workerids\"]]=\\\n",
    "#     ann_results.apply(select_worker,axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD = \"first\"\n",
    "\n",
    "# AnsAgree = evaluate_persons(ann_results[\"combine_answers\"],ann_results[\"combine_workerids\"],\n",
    "#                  stdLabels=ann_results[\"modified_answer\"],\n",
    "#                  obj=\"answer\",method=METHOD)\n",
    "\n",
    "# print(f\"Answer Agreement: {[[round(b*100,2) for b in A]for A in AnsAgree]}\")\n",
    "\n",
    "# # skip empty evidences\n",
    "# evd_input_df = copy.deepcopy(ann_results[[\"evidences\"]])\n",
    "# evd_input_df[[\"combine_evidences\",\"combine_workerids\"]]=\\\n",
    "#     ann_results.apply(remove_empty_evid,axis=1,result_type='expand') # remove empty evidences and according workers\n",
    "# evd_input_df = evd_input_df[evd_input_df[\"combine_workerids\"].apply(lambda x:x!=[])]\n",
    "\n",
    "# EvdAgree = evaluate_persons(evd_input_df[\"combine_evidences\"],evd_input_df[\"combine_workerids\"],\n",
    "#                  stdLabels=evd_input_df[\"evidences\"],\n",
    "#                  obj=\"evidence\",method=METHOD)\n",
    "\n",
    "# print(f\"skip Evidence Agreement: {[[round(b*100,2) for b in A] for A in EvdAgree]}\")\n",
    "\n",
    "# # score empty evidences as 0\n",
    "# evd_input_df = copy.deepcopy(ann_results[[\"evidences\",\"combine_workerids\"]])\n",
    "# evd_input_df[\"combine_evidences\"]=\\\n",
    "#     ann_results.apply(fill_empty_evid,axis=1) # remove empty evidences and according workers\n",
    "\n",
    "# EvdAgree = evaluate_persons(evd_input_df[\"combine_evidences\"],evd_input_df[\"combine_workerids\"],\n",
    "#                  stdLabels=evd_input_df[\"evidences\"],\n",
    "#                  obj=\"evidence\",method=METHOD)\n",
    "\n",
    "# print(f\"zero Evidence Agreement: {[[round(b*100,2) for b in A] for A in EvdAgree]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_tensorflow1]",
   "language": "python",
   "name": "conda-env-pytorch_tensorflow1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
