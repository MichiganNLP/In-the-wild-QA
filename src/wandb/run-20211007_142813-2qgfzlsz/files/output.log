
Epoch 0:   0%|                                                                                                                                      | 0/3 [00:00<?, ?it/s]
  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Epoch 0:   0%|                                                                                                                                      | 0/3 [00:00<?, ?it/s]> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(609)run_training_batch()
    608
--> 609         for split_idx, split_batch in enumerate(splits):
    610

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[ipdb> batch_output
[?7h[?12l[?25h[?2004l*** NameError: name 'batch_output' is not defined

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7ls[?7hipdb> batch_outputs
[?7h[?12l[?25h[?2004l[[]]
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(612)run_training_batch()
    611             # create an iterable for optimizers and loop over them
--> 612             for opt_idx, optimizer in self.prepare_optimizers():
    613
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(615)run_training_batch()
    614                 # toggle model params + set info to logger_connector
--> 615                 self.run_train_split_start(split_idx, split_batch, opt_idx, optimizer)
    616
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(617)run_training_batch()
    616
--> 617                 if self.should_accumulate():
    618                     # For gradient accumulation
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(642)run_training_batch()
    641                 else:
--> 642                     if self.automatic_optimization:
    643
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(644)run_training_batch()
    643
--> 644                         def train_step_and_backward_closure():
    645                             result = self.training_step_and_backward(
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(651)run_training_batch()
    650                         # optimizer step
--> 651                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
    652
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(658)run_training_batch()
    657
--> 658                     if self._curr_step_result is None:
    659                         # user decided to skip optimization



[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7lf[?7h[?12l[?25h[?25l[?7l.[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25lipdb> self._curr_step_result
[?7h[?12l[?25h[?2004l[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lx[?7h[?12l[?25h[?25l[?7li[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25ipdb> exit
Epoch 0:   0%|                                                                                                                                      | 0/3 [01:25<?, ?it/s]
Exiting Debugger.