Set SLURM handle signals.
  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.













































































Epoch 0:  82% 787/959 [02:37<00:34,  4.99it/s, loss=2.48, v_num=fl70]
Validating:   0% 0/172 [00:00<?, ?it/s]
/home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:406: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`






















































































Epoch 1:  83% 795/959 [02:38<00:32,  5.00it/s, loss=2.98, v_num=fl70]























































































Epoch 2:  82% 787/959 [02:38<00:34,  4.98it/s, loss=1.93, v_num=fl70]






















































































Epoch 3:  82% 787/959 [02:37<00:34,  4.99it/s, loss=1.46, v_num=fl70]





















































































Epoch 4:  83% 792/959 [02:39<00:33,  4.97it/s, loss=1.36, v_num=fl70]





















































































Epoch 5:  84% 801/959 [02:39<00:31,  5.01it/s, loss=1.87, v_num=fl70]




















































































Epoch 6:  82% 787/959 [02:37<00:34,  4.98it/s, loss=1.47, v_num=fl70]





















































































Epoch 7:  83% 792/959 [02:39<00:33,  4.98it/s, loss=1.01, v_num=fl70]





















































































Epoch 8:  83% 798/959 [02:39<00:32,  4.99it/s, loss=1.2, v_num=fl70]




















































































Epoch 9:  82% 787/959 [02:38<00:34,  4.98it/s, loss=1.08, v_num=fl70]







Validating:  88% 151/172 [00:11<00:01, 13.53it/s]