  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                     | 1/3 [00:00<00:01,  1.62it/s, loss=nan, v_num=ihz9]

Validating:   0%|                                                                                                                                   | 0/2 [00:00<?, ?it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 1; dropping {'val_loss_epoch': 8.225831985473633, 'val_acc_epoch': 0.11428572237491608, 'val_sentence_acc_epoch': 0.0, 'epoch': 0}.
Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 2/3 [00:00<00:00,  3.31it/s, loss=nan, v_num=ihz9]

Validating:   0%|                                                                                                                                   | 0/2 [00:00<?, ?it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 1; dropping {'val_loss_step/epoch_2': 6.8141632080078125, 'val_acc_step/epoch_2': 0.1428571492433548, 'val_sentence_acc_step/epoch_2': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 2; dropping {'val_loss_step/epoch_3': 6.8141632080078125, 'val_acc_step/epoch_3': 0.1428571492433548, 'val_sentence_acc_step/epoch_3': 0.0}.
Epoch 5:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 2/3 [00:00<00:00,  3.29it/s, loss=nan, v_num=ihz9]
Validating:   0%|                                                                                                                                   | 0/2 [00:00<?, ?it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 3; dropping {'val_loss_step/epoch_4': 6.8141632080078125, 'val_acc_step/epoch_4': 0.1428571492433548, 'val_sentence_acc_step/epoch_4': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1 < 3; dropping {'val_loss_step/epoch_4': 13.872504234313965, 'val_acc_step/epoch_4': 0.0, 'val_sentence_acc_step/epoch_4': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 4; dropping {'val_loss_step/epoch_5': 6.8141632080078125, 'val_acc_step/epoch_5': 0.1428571492433548, 'val_sentence_acc_step/epoch_5': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1 < 4; dropping {'val_loss_step/epoch_5': 13.872504234313965, 'val_acc_step/epoch_5': 0.0, 'val_sentence_acc_step/epoch_5': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 5; dropping {'val_loss_step/epoch_6': 6.8141632080078125, 'val_acc_step/epoch_6': 0.1428571492433548, 'val_sentence_acc_step/epoch_6': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1 < 5; dropping {'val_loss_step/epoch_6': 13.872504234313965, 'val_acc_step/epoch_6': 0.0, 'val_sentence_acc_step/epoch_6': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 6; dropping {'val_loss_step/epoch_7': 6.8141632080078125, 'val_acc_step/epoch_7': 0.1428571492433548, 'val_sentence_acc_step/epoch_7': 0.0}.

[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1 < 6; dropping {'val_loss_step/epoch_7': 13.872504234313965, 'val_acc_step/epoch_7': 0.0, 'val_sentence_acc_step/epoch_7': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 7; dropping {'val_loss_step/epoch_8': 6.8141632080078125, 'val_acc_step/epoch_8': 0.1428571492433548, 'val_sentence_acc_step/epoch_8': 0.0}.

Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s, loss=nan, v_num=ihz9]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 8; dropping {'val_loss_step/epoch_9': 6.8141632080078125, 'val_acc_step/epoch_9': 0.1428571492433548, 'val_sentence_acc_step/epoch_9': 0.0}.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1 < 8; dropping {'val_loss_step/epoch_9': 13.872504234313965, 'val_acc_step/epoch_9': 0.0, 'val_sentence_acc_step/epoch_9': 0.0}.