  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Epoch 0:   0%|                                                                                                                                      | 0/3 [00:00<?, ?it/s]> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py(645)run_train()
    644                     import ipdb;ipdb.set_trace()
--> 645                     self.train_loop.run_training_epoch()
    646

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?ipdb> s
[?7h[?12l[?25h[?2004l--Call--
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(473)run_training_epoch()
    472
--> 473     def run_training_epoch(self):
    474         # modify dataloader if needed (ddp, etc...)
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(475)run_training_epoch()
    474         # modify dataloader if needed (ddp, etc...)
--> 475         train_dataloader = self.trainer.accelerator.process_dataloader(self.trainer.train_dataloader)
    476
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(478)run_training_epoch()
    477         # track epoch output
--> 478         epoch_output = [[] for _ in range(self.num_optimizers)]
    479
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(480)run_training_epoch()
    479
--> 480         train_dataloader = self.trainer.data_connector.get_profiled_train_dataloader(train_dataloader)
    481         dataloader_idx = 0
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(481)run_training_epoch()
    480         train_dataloader = self.trainer.data_connector.get_profiled_train_dataloader(train_dataloader)
--> 481         dataloader_idx = 0
    482         should_check_val = False

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7l/0[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7li[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7ld[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7ld[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lr[?ipdb> train_dataloder
[?7h[?12l[?25h[?2004l*** NameError: name 'train_dataloder' is not defined
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ltrain_dataloder[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7ld[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7lr[ipdb> train_dataloader
[?7h[?12l[?25h[?2004l<generator object BaseProfiler.profile_iterable at 0x2b503b1325d0>
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ltrain_dataloader[?7h[?12l[?25h[?25l[?7l[[?7h[?12l[?25h[?25l[?7l0[?7h[?12l[?25h[?25l[?7ipdb> train_dataloader[0]
[?7h[?12l[?25h[?2004l*** TypeError: 'generator' object is not subscriptable


[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ltrain_dataloader[0][?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(482)run_training_epoch()
    481         dataloader_idx = 0
--> 482         should_check_val = False
    483
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(484)run_training_epoch()
    483
--> 484         for batch_idx, (batch, is_last_batch) in train_dataloader:
    485
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(486)run_training_epoch()
    485
--> 486             self.trainer.batch_idx = batch_idx
    487


[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?2ipdb> batch
[?7h[?12l[?25h[?2004l{'source_ids': tensor([[ 363,   19,    8,  ...,    0,    0,    0],
        [ 363,   97,   19,  ...,    0,    0,    0],
        [ 366,   33,    8,  ...,    0,    0,    0],
        ...,
        [2645,   19,    8,  ...,    0,    0,    0],
        [2840,   33,   79,  ...,    0,    0,    0],
        [ 571,  186,  502,  ...,    0,    0,    0]]), 'source_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'target_ids': tensor([[13423,     1,     0,  ...,     0,     0,     0],
        [  431,    10,  2445,  ...,     0,     0,     0],
        [  431,    10,  2445,  ...,     0,     0,     0],
        ...,
        [ 2353,     1,     0,  ...,     0,     0,     0],
        [ 1953,     1,     0,  ...,     0,     0,     0],
        [  204,     1,     0,  ...,     0,     0,     0]]), 'target_mask': tensor([[1, 1, 0,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 0,  ..., 0, 0, 0],
        [1, 1, 0,  ..., 0, 0, 0],
        [1, 1, 0,  ..., 0, 0, 0]])}
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(491)run_training_epoch()
    490             # ------------------------------------
--> 491             with self.trainer.profiler.profile("run_training_batch"):
    492                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(492)run_training_epoch()
    491             with self.trainer.profiler.profile("run_training_batch"):
--> 492                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
    493
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/In-the-wild-QA/src/T5/model.py(125)optimizer_step()
    124         import ipdb;ipdb.set_trace()
--> 125         assert not self.hparams.use_tpu
    126         optimizer.step()
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lp[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h
[?7h[?12l[?25h[?2004l> /home/dnaihao/In-the-wild-QA/src/T5/model.py(126)optimizer_step()
    125         assert not self.hparams.use_tpu
--> 126         optimizer.step()
    127         optimizer.zero_grad()
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
                                                           [?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/In-the-wild-QA/src/T5/model.py(128)optimizer_step()
    127         optimizer.zero_grad()
--> 128         self.lr_scheduler.step()
    129
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l--Return--
None
> /home/dnaihao/In-the-wild-QA/src/T5/model.py(128)optimizer_step()
    127         optimizer.zero_grad()
--> 128         self.lr_scheduler.step()
    129
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l--Return--
None
> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(434)optimizer_step()
    433             using_native_amp=using_native_amp,
--> 434             using_lbfgs=is_lbfgs,
    435         )

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(660)run_training_batch()
    659                         # make sure to zero grad.
--> 660                         continue
    661
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(611)run_training_batch()
    610             # create an iterable for optimizers and loop over them
--> 611             for opt_idx, optimizer in self.prepare_optimizers():
    612

[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(674)run_training_batch()
    673
--> 674         result = AttributeDict(
    675             signal=0,
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(675)run_training_batch()
    674         result = AttributeDict(
--> 675             signal=0,
    676             grad_norm_dic=grad_norm_dic,
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(676)run_training_batch()
    675             signal=0,
--> 676             grad_norm_dic=grad_norm_dic,
    677             training_step_output_for_epoch_end=batch_outputs,
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ln[?7h[?12l[?25h[?ipdb> n
                                                           [?2ipdb>
[?7h[?12l[?25h[?2004l> /home/dnaihao/anaconda3/envs/442/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py(679)run_training_batch()
    678         )
--> 679         return result
    680


[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lr[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?ipdb> result
[?7h[?12l[?25h[?2004l"grad_norm_dic":                      {}
"signal":                             0
"training_step_output_for_epoch_end": [[]]
[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7lb[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h




[?2004h[?25l[?7lipdb> [?7h[?12l[?25h[?25l[?7l[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7lf[?7h[?12l[?25h[?25l[?7l.[?7h[?12l[?25h[?25l[?7ls[?7h[?12l[?25h[?25l[?7lh[?7h[?12l[?25h[?25l[?7lo[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7ld[?7h[?12l[?25h[?25l[?7l_[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lc[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7lm[?7h[?12l[?25h[?25l[?7lu[?7h[?12l[?25h[?25l[?7ll[?7h[?12l[?25h[?25l[?7la[?7h[?12l[?25h[?25l[?7lt[?7h[?12l[?25h[?25l[?7le[?7h[?12l[?25h[?25l[?7l([?7h[?12l[?25h[?2ipdb> self.should_accumulate()
[?7h[?12l[?25h[?2004lFalse
